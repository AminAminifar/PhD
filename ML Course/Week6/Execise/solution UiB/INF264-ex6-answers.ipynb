{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "from random import random\n",
    "from random import randrange\n",
    "import random\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split,KFold,cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score, precision_score\n",
    "from sklearn.utils.testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random subsample from the dataset with replacement\n",
    "def subsample(X,y, ratio=1.0):\n",
    "    sample_X = list()\n",
    "    sample_y = list()\n",
    "    n_sample = round(len(X) * ratio)\n",
    "    while len(sample_X) < n_sample:\n",
    "        index = randrange(len(X))\n",
    "        sample_X.append(X[index])\n",
    "        sample_y.append(y[index])\n",
    "    return sample_X,sample_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap Aggregation Algorithm\n",
    "def bagging(X_train,y_train,X_test, n_clfs,Classifier):\n",
    "    clfs = list()\n",
    "    for i in range(n_clfs):\n",
    "        sample_X,sample_y = subsample(X_train,y_train, sample_size)\n",
    "        clf = Classifier(random_state=seed).fit(sample_X,sample_y)\n",
    "        clfs.append(clf)\n",
    "    y_= []\n",
    "    for row in X_test:\n",
    "        predictions = [clf.predict([row])[0] for clf in clfs]\n",
    "        y_.append(max(set(predictions), key=predictions.count))\n",
    "    return(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KFold_split(X, y, num_folds, seed):\n",
    "    KFold_splitter = KFold(n_splits=num_folds, shuffle=True, random_state=seed)\n",
    "    X_train_folds = []\n",
    "    X_val_folds = []\n",
    "    y_train_folds = []\n",
    "    y_val_folds = []\n",
    "    for (kth_fold_train_idxs, kth_fold_val_idxs) in KFold_splitter.split(X, y):\n",
    "        X_train_folds.append(X[kth_fold_train_idxs])\n",
    "        X_val_folds.append(X[kth_fold_val_idxs])\n",
    "        y_train_folds.append(y[kth_fold_train_idxs])\n",
    "        y_val_folds.append(y[kth_fold_val_idxs])\n",
    "    return X_train_folds, X_val_folds, y_train_folds, y_val_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_algorithm(X_train_val, y_train_val, num_folds, seed, algorithm,*args):\n",
    "    # Extract train and validation folds:\n",
    "    X_train_folds, X_val_folds, y_train_folds, y_val_folds = KFold_split(X_train_val, y_train_val, num_folds, seed)\n",
    "    scores = list()\n",
    "    \n",
    "    for X_train_fold, X_val_fold, y_train_fold, y_val_fold in zip(X_train_folds, X_val_folds\n",
    "                                                                  , y_train_folds, y_val_folds):\n",
    "        predictions = algorithm(X_train_fold,y_train_fold,X_val_fold,*args)\n",
    "        scores.append(np.sum(list(predictions) == y_val_fold) / float(len(y_val_fold)))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trees: 1\n",
      "Scores: [0.6896551724137931, 0.7931034482758621, 0.7241379310344828, 0.41379310344827586, 0.7586206896551724]\n",
      "Mean Accuracy: 0.676\n",
      "Test set Accuracy: 0.714\n",
      "Trees: 5\n",
      "Scores: [0.6551724137931034, 0.7586206896551724, 0.7586206896551724, 0.7931034482758621, 0.8275862068965517]\n",
      "Mean Accuracy: 0.759\n",
      "Test set Accuracy: 0.778\n",
      "Trees: 10\n",
      "Scores: [0.7586206896551724, 0.8275862068965517, 0.7586206896551724, 0.8275862068965517, 0.7241379310344828]\n",
      "Mean Accuracy: 0.779\n",
      "Test set Accuracy: 0.794\n",
      "Trees: 20\n",
      "Scores: [0.7586206896551724, 0.7931034482758621, 0.7931034482758621, 0.896551724137931, 0.7586206896551724]\n",
      "Mean Accuracy: 0.800\n",
      "Test set Accuracy: 0.810\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test bagging on the sonar dataset\n",
    "seed = 2\n",
    "# load and prepare data\n",
    "filename = 'sonar.all-data'\n",
    "dataset = pd.read_csv(filename,header=None)\n",
    "X = dataset.iloc[:,:-1].to_numpy()\n",
    "y = (dataset.iloc[:,-1].to_numpy()=='M').astype(int)\n",
    "# evaluate algorithm\n",
    "num_folds = 5\n",
    "sample_size = 0.8\n",
    "random.seed(seed)\n",
    "# Extract a test set:\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=seed)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# For each hyper-parameter instance, do KFold cross validation:\n",
    "for n_trees in [1, 5, 10, 20]:\n",
    "    scores = evaluate_algorithm(X_train_val, y_train_val, num_folds, seed, bagging,n_trees,DecisionTreeClassifier)\n",
    "    print('Trees: %d' % n_trees)\n",
    "    print('Scores: %s' % scores)\n",
    "    print('Mean Accuracy: %.3f' % (sum(scores)/float(len(scores))))\n",
    "    print('Test set Accuracy: %.3f' %(accuracy_score(bagging(X_train_val,y_train_val,X_test,n_trees\n",
    "                                                             ,DecisionTreeClassifier),y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_diabetes():\n",
    "    dataset = pd.read_csv('pima-indians-diabetes.csv', header=None)\n",
    "    print(dataset.describe())\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0           1           2           3           4           5  \\\n",
      "count  768.000000  768.000000  768.000000  768.000000  768.000000  768.000000   \n",
      "mean     3.845052  120.894531   69.105469   20.536458   79.799479   31.992578   \n",
      "std      3.369578   31.972618   19.355807   15.952218  115.244002    7.884160   \n",
      "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "25%      1.000000   99.000000   62.000000    0.000000    0.000000   27.300000   \n",
      "50%      3.000000  117.000000   72.000000   23.000000   30.500000   32.000000   \n",
      "75%      6.000000  140.250000   80.000000   32.000000  127.250000   36.600000   \n",
      "max     17.000000  199.000000  122.000000   99.000000  846.000000   67.100000   \n",
      "\n",
      "                6           7           8  \n",
      "count  768.000000  768.000000  768.000000  \n",
      "mean     0.471876   33.240885    0.348958  \n",
      "std      0.331329   11.760232    0.476951  \n",
      "min      0.078000   21.000000    0.000000  \n",
      "25%      0.243750   24.000000    0.000000  \n",
      "50%      0.372500   29.000000    0.000000  \n",
      "75%      0.626250   41.000000    1.000000  \n",
      "max      2.420000   81.000000    1.000000  \n",
      "     0    1   2   3    4     5      6   7  8\n",
      "0    6  148  72  35    0  33.6  0.627  50  1\n",
      "1    1   85  66  29    0  26.6  0.351  31  0\n",
      "2    8  183  64   0    0  23.3  0.672  32  1\n",
      "3    1   89  66  23   94  28.1  0.167  21  0\n",
      "4    0  137  40  35  168  43.1  2.288  33  1\n",
      "5    5  116  74   0    0  25.6  0.201  30  0\n",
      "6    3   78  50  32   88  31.0  0.248  26  1\n",
      "7   10  115   0   0    0  35.3  0.134  29  0\n",
      "8    2  197  70  45  543  30.5  0.158  53  1\n",
      "9    8  125  96   0    0   0.0  0.232  54  1\n",
      "10   4  110  92   0    0  37.6  0.191  30  0\n",
      "11  10  168  74   0    0  38.0  0.537  34  1\n",
      "12  10  139  80   0    0  27.1  1.441  57  0\n",
      "13   1  189  60  23  846  30.1  0.398  59  1\n",
      "14   5  166  72  19  175  25.8  0.587  51  1\n",
      "15   7  100   0   0    0  30.0  0.484  32  1\n",
      "16   0  118  84  47  230  45.8  0.551  31  1\n",
      "17   7  107  74   0    0  29.6  0.254  31  1\n",
      "18   1  103  30  38   83  43.3  0.183  33  0\n",
      "19   1  115  70  30   96  34.6  0.529  32  1\n"
     ]
    }
   ],
   "source": [
    "# print the first 20 rows of data\n",
    "dataset = load_diabetes()\n",
    "print(dataset.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1      5\n",
      "2     35\n",
      "3    227\n",
      "4    374\n",
      "5     11\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#TODO\n",
    "#Count the number of zero values in columns [1,2,3,4,5]\n",
    "print((dataset[[1,2,3,4,5]] == 0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0\n",
      "1      5\n",
      "2     35\n",
      "3    227\n",
      "4    374\n",
      "5     11\n",
      "6      0\n",
      "7      0\n",
      "8      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#TODO\n",
    "# mark zero values as missing or NaN\n",
    "dataset[[1,2,3,4,5]] = dataset[[1,2,3,4,5]].replace(0, np.NaN)\n",
    "# count the number of NaN values in each column\n",
    "print(dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0      1     2     3      4     5      6   7  8\n",
      "0    6  148.0  72.0  35.0    NaN  33.6  0.627  50  1\n",
      "1    1   85.0  66.0  29.0    NaN  26.6  0.351  31  0\n",
      "2    8  183.0  64.0   NaN    NaN  23.3  0.672  32  1\n",
      "3    1   89.0  66.0  23.0   94.0  28.1  0.167  21  0\n",
      "4    0  137.0  40.0  35.0  168.0  43.1  2.288  33  1\n",
      "5    5  116.0  74.0   NaN    NaN  25.6  0.201  30  0\n",
      "6    3   78.0  50.0  32.0   88.0  31.0  0.248  26  1\n",
      "7   10  115.0   NaN   NaN    NaN  35.3  0.134  29  0\n",
      "8    2  197.0  70.0  45.0  543.0  30.5  0.158  53  1\n",
      "9    8  125.0  96.0   NaN    NaN   NaN  0.232  54  1\n",
      "10   4  110.0  92.0   NaN    NaN  37.6  0.191  30  0\n",
      "11  10  168.0  74.0   NaN    NaN  38.0  0.537  34  1\n",
      "12  10  139.0  80.0   NaN    NaN  27.1  1.441  57  0\n",
      "13   1  189.0  60.0  23.0  846.0  30.1  0.398  59  1\n",
      "14   5  166.0  72.0  19.0  175.0  25.8  0.587  51  1\n",
      "15   7  100.0   NaN   NaN    NaN  30.0  0.484  32  1\n",
      "16   0  118.0  84.0  47.0  230.0  45.8  0.551  31  1\n",
      "17   7  107.0  74.0   NaN    NaN  29.6  0.254  31  1\n",
      "18   1  103.0  30.0  38.0   83.0  43.3  0.183  33  0\n",
      "19   1  115.0  70.0  30.0   96.0  34.6  0.529  32  1\n"
     ]
    }
   ],
   "source": [
    "print(dataset.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(392, 9)\n"
     ]
    }
   ],
   "source": [
    "#TODO\n",
    "#delete rows contating NAN values from the dataset using .dropna(inplace = True) built-in function\n",
    "dataset.dropna(inplace=True)\n",
    "print (dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = dataset.values\n",
    "X = values[:,0:8]\n",
    "y = values[:,8]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle=True, random_state=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "#complete the function to \n",
    "#evaluate the MLP model on the test set\n",
    "def evaluate_MLP(X_train, y_train,X_test,y_test,seed=7):\n",
    "    model = MLPClassifier(random_state=seed).fit(X_train,y_train)\n",
    "    result = model.predict(X_test)\n",
    "    print(accuracy_score(model.predict(X_test),y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7040816326530612\n"
     ]
    }
   ],
   "source": [
    "evaluate_MLP(X_train, y_train,X_test,y_test,7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0           1           2           3           4           5  \\\n",
      "count  768.000000  768.000000  768.000000  768.000000  768.000000  768.000000   \n",
      "mean     3.845052  120.894531   69.105469   20.536458   79.799479   31.992578   \n",
      "std      3.369578   31.972618   19.355807   15.952218  115.244002    7.884160   \n",
      "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "25%      1.000000   99.000000   62.000000    0.000000    0.000000   27.300000   \n",
      "50%      3.000000  117.000000   72.000000   23.000000   30.500000   32.000000   \n",
      "75%      6.000000  140.250000   80.000000   32.000000  127.250000   36.600000   \n",
      "max     17.000000  199.000000  122.000000   99.000000  846.000000   67.100000   \n",
      "\n",
      "                6           7           8  \n",
      "count  768.000000  768.000000  768.000000  \n",
      "mean     0.471876   33.240885    0.348958  \n",
      "std      0.331329   11.760232    0.476951  \n",
      "min      0.078000   21.000000    0.000000  \n",
      "25%      0.243750   24.000000    0.000000  \n",
      "50%      0.372500   29.000000    0.000000  \n",
      "75%      0.626250   41.000000    1.000000  \n",
      "max      2.420000   81.000000    1.000000  \n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "5    0\n",
      "6    0\n",
      "7    0\n",
      "8    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dataset = load_diabetes()\n",
    "#TODO\n",
    "# Mark zero values of columns [1,2,3,4,5] as missing or NaN\n",
    "# This time fill missing values with mean column values using .fillna(dataset.mean(),inplace=True)\n",
    "\n",
    "dataset[[1,2,3,4,5]] = dataset[[1,2,3,4,5]].replace(0, np.NaN)\n",
    "dataset.fillna(dataset.mean(), inplace=True)\n",
    "# count the number of NaN values in each column\n",
    "print(dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148.0</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>35.00000</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>33.600000</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85.0</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>29.00000</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>26.600000</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183.0</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>23.300000</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89.0</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>23.00000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>28.100000</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>35.00000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>43.100000</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>116.0</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>25.600000</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>78.0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>32.00000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.248</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>115.0</td>\n",
       "      <td>72.405184</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>35.300000</td>\n",
       "      <td>0.134</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>197.0</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>45.00000</td>\n",
       "      <td>543.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>0.158</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>125.0</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>32.457464</td>\n",
       "      <td>0.232</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>110.0</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>37.600000</td>\n",
       "      <td>0.191</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>168.0</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>0.537</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10</td>\n",
       "      <td>139.0</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>27.100000</td>\n",
       "      <td>1.441</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>189.0</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>23.00000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>30.100000</td>\n",
       "      <td>0.398</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>166.0</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>19.00000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>25.800000</td>\n",
       "      <td>0.587</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7</td>\n",
       "      <td>100.0</td>\n",
       "      <td>72.405184</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.484</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>47.00000</td>\n",
       "      <td>230.000000</td>\n",
       "      <td>45.800000</td>\n",
       "      <td>0.551</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7</td>\n",
       "      <td>107.0</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>29.600000</td>\n",
       "      <td>0.254</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>103.0</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>38.00000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>43.300000</td>\n",
       "      <td>0.183</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>115.0</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>30.00000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>34.600000</td>\n",
       "      <td>0.529</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0      1          2         3           4          5      6   7  8\n",
       "0    6  148.0  72.000000  35.00000  155.548223  33.600000  0.627  50  1\n",
       "1    1   85.0  66.000000  29.00000  155.548223  26.600000  0.351  31  0\n",
       "2    8  183.0  64.000000  29.15342  155.548223  23.300000  0.672  32  1\n",
       "3    1   89.0  66.000000  23.00000   94.000000  28.100000  0.167  21  0\n",
       "4    0  137.0  40.000000  35.00000  168.000000  43.100000  2.288  33  1\n",
       "5    5  116.0  74.000000  29.15342  155.548223  25.600000  0.201  30  0\n",
       "6    3   78.0  50.000000  32.00000   88.000000  31.000000  0.248  26  1\n",
       "7   10  115.0  72.405184  29.15342  155.548223  35.300000  0.134  29  0\n",
       "8    2  197.0  70.000000  45.00000  543.000000  30.500000  0.158  53  1\n",
       "9    8  125.0  96.000000  29.15342  155.548223  32.457464  0.232  54  1\n",
       "10   4  110.0  92.000000  29.15342  155.548223  37.600000  0.191  30  0\n",
       "11  10  168.0  74.000000  29.15342  155.548223  38.000000  0.537  34  1\n",
       "12  10  139.0  80.000000  29.15342  155.548223  27.100000  1.441  57  0\n",
       "13   1  189.0  60.000000  23.00000  846.000000  30.100000  0.398  59  1\n",
       "14   5  166.0  72.000000  19.00000  175.000000  25.800000  0.587  51  1\n",
       "15   7  100.0  72.405184  29.15342  155.548223  30.000000  0.484  32  1\n",
       "16   0  118.0  84.000000  47.00000  230.000000  45.800000  0.551  31  1\n",
       "17   7  107.0  74.000000  29.15342  155.548223  29.600000  0.254  31  1\n",
       "18   1  103.0  30.000000  38.00000   83.000000  43.300000  0.183  33  0\n",
       "19   1  115.0  70.000000  30.00000   96.000000  34.600000  0.529  32  1"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = dataset.values\n",
    "X = values[:,0:8]\n",
    "y = values[:,8]\n",
    "X_train, _, y_train, _ = train_test_split(X, y, test_size=0.25, shuffle=True, random_state=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(576, 8)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "evaluate_MLP(X_train, y_train,X_test,y_test,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 31)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('creditcard.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    284315\n",
      "1       492\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.Class.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHwAAAHDCAYAAABWLA0oAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3Xm8lnPi//F3paLtWxJRthkUCaGFVIx9+8o6XxrrMDKMZcYW853FbwjfDEpNlkjGPmoMMyRLiDYq40cYGVGiqJSi9fz+8D337xztKnF5Ph+PeTzOue/rvu7Pdc59n3G/+lyfq0pZWVlZAAAAACiMqut6AAAAAACsWYIPAAAAQMEIPgAAAAAFI/gAAAAAFIzgAwAAAFAwgg8AAABAwQg+ACxVs2bN0qxZs5x44onreihfy6WXXlo6hkmTJi1x/8iRI0v39+rVax2McM0oynF8U6ZMmZI//OEPOeSQQ9KqVavSz+6II45Y10PjO2jWrFml19DPf/7zdT0cAKhkvXU9AAC+vmbNmi319urVq6dOnTqpW7duNttss+ywww7Zeeeds/fee2f99df/hke5dOVxokmTJjnqqKPW8Wi+PWbNmpU777wzSbL99ttnv/32W8cjKo4JEybk+OOPz6effvq1Hj9w4MB069ZtjYzlqaeeStOmTdfIvlizbrnllsybNy8bbbRRjj/++HU9HAD42gQfgAJasGBBZsyYkRkzZuS9997LiBEjkiT16tVL586dc+6556Zu3brrdIw33XRTkqRNmzaCTwWzZs0q/WyOPPJIwWcN+p//+Z9S7Nlnn32y7777pkGDBkmSOnXqrMuh8S1yyy23ZPbs2WnevLngA8B3muADUBC9e/cufV1WVpbZs2dn1qxZGT9+fEaPHp3Jkydn1qxZGTBgQJ544olcd9112X333Ze5vzfffPObGPZac/XVV+fqq69e18NY69q2bfud/119ExYsWJAXX3wxSfLDH/4wffr0SdWqq3Zme7t27Sq9z75qwIABGTlyZJLkxBNPTLt27Za5bcOGDVfpufl2qlevnvcfAN9agg9AQSxvJkhZWVmee+65XHXVVXn33Xfz4Ycf5swzz8x9992Xbbfd9hscJawbM2bMyLx585IkzZs3X+XYkySbbbZZNttss2Xe/+STT5a+3mGHHczOAgDWKYs2A3wPVKlSJZ06dcpDDz2U3XbbLUny2Wef5bzzzsvixYvX8ehg7Zs/f37p6xo1aqzDkQAAfDPM8AH4HqlTp05uuOGGHHrooZk1a1YmTJiQf/zjHznssMOW2LZ8Qeg2bdrkrrvuWur+Pvroo9x333158cUX8+9//ztz5sxJrVq10qBBg2y88cZp165dOnbsmJ122mmJ/ZYbNWrUUhefHjBgQNq2bZvkyytRnXTSSUmSc845J7/4xS/yzjvv5N57782wYcPy0UcfZc6cOenevXtpPaBLL700gwYNSrLyC+S++eab+fOf/5zhw4dn6tSpqVWrVpo3b55jjjlmqT+jchUX8604hlXZdtKkSdl3330rbTto0KDSMVRU8XiW9rNZls8//zz3339/nnrqqbzzzjv59NNPU7du3Wy11VbZe++9c8IJJyx3badevXqV1hcq//2MGzcud911V8aMGZNp06albt262WmnndKlS5d07NhxmftaVTNnzszdd9+d5557LhMnTsxnn32W+vXrZ5tttsm+++6b4447LjVr1lzicRVfB+WW9nNdV4soP/nkkzn77LOTJN26dcspp5ySN998M/fee2/pdTh37tz07t270oyhcePG5bnnnsvYsWMzYcKEzJgxI1WrVs2GG26Yli1b5uCDD86BBx643JlM/fv3T/fu3ZOktP/XX389d911V0aOHJlp06alVq1a2XHHHXP88cevcMbSnDlz8uCDD+bpp5/O22+/nVmzZqV69epp0KBBNtpoo+y6667p0KFD2rVrl2rVqi3x+DVxTBVNnz49999/f+nv08yZM1O9evVsuumm2XHHHUvrOJUHwN133z2zZ88uPf6NN95Y6t+mir+LWbNmpXXr1kmSfffdN3369FnmeBYtWpRHHnkkgwcPzmuvvZYZM2Zkgw02yGabbZY999wzXbp0SZMmTZb5+KW9ViZOnJgBAwbk+eefz4cffpiaNWumWbNm6dy5c4466qjl/qwWLFiQv/71rxk8eHDefPPNzJw5M1WrVk2DBg2y4YYbZqeddkqHDh2y1157LfW9BcC3n+AD8D2z8cYb57jjjsttt92WJHnooYeWGzOWZejQobngggsyd+7cSrfPmjUrs2bNysSJEzN69Oj0798/L7300hoZe7m//vWv+e1vf5svvvhije7zv//7vyvNBJk3b16GDx+e4cOH55FHHknPnj2/sx98xo0bl1/84heZOnVqpdunT5+e6dOnZ8yYMbn99ttz3XXXZa+99lqpffbt2zc33nhjpVli06dPz9ChQzN06NCcffbZOffcc1d77E8++WS6deuWWbNmVbp92rRpmTZtWoYPH57bb789vXv3zg477LDaz7cu3X333enevXsWLFiwzG2uuuqq0pXcvuqDDz7IBx98kMGDB2f33XdPr169suGGG67Uc9911125+uqrs3DhwtJt8+fPz7BhwzJs2LCceOKJ+fWvf73Ux/7rX//K6aefng8//LDS7QsWLMjcuXMzefLkvPLKK7njjjuWGtfW9DHdfffd6dGjxxJ/nxYsWJAJEyZkwoQJefjhh3PZZZfl5JNPXu6+1oQPPvggP//5zzN+/PhKt8+fPz+ffvppxo8fnz//+c+55JJL0qVLl5Xa5+OPP55u3bpVOsZ58+Zl9OjRGT16dIYOHZobb7xxqXHto48+yumnn5633nprifumTJmSKVOm5LXXXsu9995bKb4D8N0i+AB8Dx1++OGl4DN27NgsWLAg1atXX+nHf/TRR5Viz957750999wzG2+8ccrKyvLJJ5/kjTfeyIsvvljpX8yT/7+4dPm/VG+77bY5//zzl3iOZa0tNGbMmPTt2zdVq1bNMccck1133TU1a9bMv//972y00UYrfQwVvfrqq7n55puTJEcffXRat26dqlWr5tVXX81DDz2UuXPnZujQobnooovSs2fPr/UcK9KwYcP07t07n3zySX7zm98k+XJB5vLZO1/ddlWMHz8+J598cimQ7bDDDjnssMOy6aab5uOPP85jjz2WMWPGZObMmenatWv69eu3wg94DzzwQB599NFssskmOfLII7PttttmwYIFef755/OPf/wjZWVl6d27d1q3bp099thjlcZb0bPPPptzzz03ixYtSpK0bt06Bx54YBo2bJgPPvggDz/8cN5666188MEH+clPfpIHH3wwP/zhD0uPP/HEE7Pffvut8Of6bVhE+fnnn88LL7yQDTbYIMccc0x23nnnrLfeenn77bdTv3790nZffPFFqlevnt122y277LJLtthii9SqVSszZszI+++/n4cffjiffPJJXnrppZx//vnp37//CmfFPPbYY3n00UfTsGHDHHXUUWnWrFkWLVqUESNG5OGHH87ixYtz1113pU2bNjnggAMqPXbBggU555xzSrFn5513zn777ZcmTZqkatWq+fTTT/P2229n5MiRSw0Ma/qYbrjhhvzpT38qfd+2bdvsvffeady4cRYsWJD3338/I0eOzEsvvZSysrLSdj169MjChQtz4YUX5vPPP0+TJk1y2WWXLbH/ijMWV8b06dNzwgknZMqUKUmSJk2a5Mgjj8wPfvCDzJkzJ88++2yefPLJzJs3L1dccUWSrDD6jBkzJk8//XRq1KiRn/zkJ2nZsmXWW2+9jB07Ng888EDmz5+fIUOGZMCAATn11FOXePyFF15Y+l1ss802Ofjgg7PlllumevXqmT17dt55552MGjUqr7322iodKwDfLoIPwPfQtttum1q1amXu3Ln5/PPPM2HChDRv3nylH//oo4+WYs+FF16YM844Y6nblZWV5eWXX65021dPC2nQoMEqLW774osvplGjRunfv3+22WablX7c8jz77LOpXbt2br/99uyyyy6l24844oj85Cc/yYknnpipU6dm8ODBGTx4cA488MA18rwVbbDBBtlvv/0yadKk0m2bbbbZai/8u3jx4lx00UWl2HPSSSelW7dulT4sn3TSSendu3d69uyZBQsW5JJLLsngwYOXO5vp0UcfTfv27XPTTTelVq1apduPPPLI7LTTTqVThW6//favHXw+++yzdOvWrRR7Lrnkkpx22mmVtjnllFPy+9//Pg888EDmzJmTiy++OA899FDp/hYtWqRFixZr/Oe6NgwbNixNmjTJnXfemc0333yZ2x155JE5//zzlznL5fzzz8/vfve7DBw4MCNHjszTTz+9wuN99NFH06pVq9xyyy2pV69e6fbOnTundevWpfBx++23LxF8Ro8enXfffTdJcthhh6VHjx6pUqXKUp9n/PjxleLVmj6m559/Pn379k2S1KpVK9dff3323nvvJbY755xzMmnSpEqzxsq3W2+9L//zuG7dumvkdfKHP/yhFHs6dOiQnj17VnrPHHfccXniiSdywQUXZOHChbnmmmvSoUOHbLHFFsvc5+DBg7P11lvnjjvuyKabblq6/bDDDst+++2XU089NWVlZenfv39OPvnkSu/3iRMnZtSoUUm+PGW3X79+y1zXauLEidlggw1W6/gBWHcs2gzwPVStWrVssskmpe+nT5++So+fOHFi6evjjjtumdtVqVJluZd+/7p+//vfr7HYU+7iiy+uFHvKbbXVVrnyyitL399+++1r9HnXtmeeeSb/+te/kiS77LJLLrvssqXOjDj77LNLH3inTJmSv/3tb8vdb/369XP99ddX+uBa7qSTTipdzWrEiBGVThFaFQMHDswnn3ySJDn44IOXiD3Jlx/Of/e735WC5f/9v/+3dPn176JrrrlmubEnSVq1arXcU5pq1qyZ3//+96VtVvS7TL6MIz179qwUe8odffTR2W677ZJ8eWrgZ599Vun+in8Pjj322GXGniTZfvvtU6dOnSVuX1PHdOONN5Zm7Vx55ZVLjT3lmjZtutZPAXz//ffz2GOPJflyFtkf//jHpb5nDjjggJx++ulJvjwtq3///svdb5UqVXLDDTdUij3l9thjj3Tq1ClJ8uGHH5be/+Uq/r46d+683EXMt9xyy2y88cbLHQsA316CD8D31H/8x3+Uvp45c+YqPbbiv/h+9cPE2takSZP86Ec/WqP7/I//+I/lLrTcsWPHUmAaN25cpk2btkaff20aMmRI6euf/vSny/0wXnGm1hNPPLHc/Xbu3LnSa6iiqlWrlhaynT9/ft57771VGXJJxbEvaxZZ8mXArBiDVjT2b6tmzZqVfm6rq0aNGmnRokWS5JVXXlnh9gcddNByP9iXn+JXVlaWCRMmVLqv4t+Dt99+++sMd6Ws6JjefffdvPrqq0m+DEuHHHLIWhvLynrqqadKa1z9+Mc/XmpQK3fKKaeUTq1d0Wu4TZs2y52V2a5du9LX6+r3BcC655QugO+p1bkc+5577ln6F+hf/OIXOfPMM3PQQQelcePGa2h0y7brrrsuN1p8HbvtttsKL9Xdrl270oejV199dY1Hp7Xln//8Z5IvZwTsueeey9121113LZ3qV/64Zdl5552Xe3/FGWRfXWx5ZZSVlZU+vDdo0KD0QX9ZOnToUPp6RWP/tlqV2XALFy7ME088kSFDhuT111/Pxx9/nLlz5y71fT116tSUlZUt932ztNltFVWMQZ9++mml+9q2bZv11lsvCxcuzLXXXpupU6fmiCOOqLSW0jdxTBVPH/22vD/LX8NJ0r59++Vu26BBg+y4444ZO3Zspk2blilTpix1Bk+y4t9XxfffV39fO+64Y+rVq5dZs2alf//++eKLL3L00UenRYsWa/xvKwDrluAD8D1V8UP40tbUWJ5OnTrlsMMOy6OPPprp06ene/fu6d69e7baaqu0atUqu+++e/bZZ5+1shBuxQ8ya8qWW265wm0qrqfx1StdfZuVz0baaKONlnoqTUVVq1bNFltskTfeeCMzZ87M/PnzlxnCGjRosNx9VXzcvHnzVnHUX67f8/nnnyf58rS6Fdlwww1Tt27dzJ49+zv1+6loZV/bEydOzNlnn73Ss+sWL16cOXPmLPf3vyq/z4pXskuSTTfdNOeff3569OiRefPm5eabb87NN9+cRo0aZdddd83uu++eTp06Lfd9tiaOqeIVwlY1Nq0tFV+LW2+99Qq332qrrTJ27NgkX753lxV8Vuf9t8EGG+S3v/1tLr744ixatCj33HNP7rnnntSvXz+tWrXKbrvtlg4dOqzSum4AfDsJPgDfQ4sWLcpHH31U+n5lL9tcUY8ePdKuXbvceeedpQ9p7777bt59990MGjQo1apVy8EHH5xLLrlkja4Bsf7666+xfa3KPiuuu/HVSz1/m82ZMydJlrpuyNJU3G7OnDnLDD4ruurT6iofd5KVXjS2Vq1amT17dqXHfpcsb5HscnPnzs2pp56ayZMnJ0kaNWqUffbZJ9tss00aNmyYmjVrlmZp3HzzzaXZTiua0be6v88zzjgjzZo1y80335yXX345ZWVlmTZtWmmh8yuvvDLt2rVLt27dlggJa+qYKq4ttLKv97VtVV/HX33/LcvqzsQ57LDDsvnmm+emm27KCy+8kEWLFmXmzJl55pln8swzz6RHjx5p0aJFLr300rRp02a1nguAdUfwAfgeeuutt0qzJ2rVqvW1/jW8SpUqOfbYY3Psscfm/fffz8svv5wxY8Zk5MiReffdd7No0aI8+uijefnll/OXv/zla18y/ZtQfgWr5akYeVbnw+TqnEr3ddSuXTuzZs1a6UhVcbvatWuvrWGtUMXnLn+trkj52NfluNe2v/zlL6Uwsv/+++ePf/zjMqPc3Xff/U0OLR07dkzHjh1Ll08fO3ZsRo0alddffz1lZWUZMWJEfvzjH+fPf/5zWrZsWXrcmjqmirN9vi1R9quv4xX97fgm338777xzbr311syaNSsvv/xyxo0bl1GjRuWVV17JokWL8tprr+Xkk09O7969vzWnyAGwaizaDPA99Mgjj5S+btWqVWmh0K9r8803T+fOnXPFFVdk8ODBGThwYOnqN1OmTEm/fv1Wa/9rW8Wr1ixLxYWHvzpjqeKH0wULFix3PzNmzFjF0a2eRo0aJUk+/vjjJa6u9FVlZWWl46xfv/4K1zVam+rUqVOaEVF+ye/lmT59embPnp1kyd9PkVS8Atmvf/3r5f6OPvjgg29iSEto2LBhDjzwwFx66aUZOHBgnnzyyVIw+OKLL3LddddV2n5NHVPFNcS+ulDxulLxtbgyr+OK23xTr+N69epln332yQUXXJB77703zz33XOnqi4sXL0737t2/kXEAsOYJPgDfM1OnTs2DDz5Y+v6YY45Z48/RokWLXHvttaXvKy6mWq78lITySyivS2PGjFliXZKvGjlyZOnrirMTkqRu3bqlr1e0fsyKFhSueGrNmvjZ7LTTTqV9jRgxYrnbjhkzpjTDoPxx60qVKlVKP+cZM2Zk/Pjxy91+2LBhpa/X9djXpvLL1NeoUWO5i6S///77KxUYvglNmzbNDTfcUJrd8tW/B2vqmCouev30009/7fGWvwfXxPuv4t+KimFraWbOnJnXXnstyZeh9ptYBH9pNtpoo1xxxRWltbPee++979SVCQH4/wQfgO+Rzz77LOeff35pweYf/vCHOeigg9bKczVp0qT09cKFC5e4v/zD37fh1IuZM2fmr3/96zLvHzZsWGmdolatWpVmzZQrv2R7kuVGlffffz/PPPPMcsdS8ZSPlT2VaXkOOOCA0tf9+vVb7ofYW2+9damPW1cqjuG2225b5naLFi3KHXfcUfr+wAMPXKvjWpfK15uaP39+pXW4vqpPnz7f1JBWSs2aNUvvm8WLF1d6Ha6pY9pyyy1LV48bP358/vGPf3ytsZa/B9fE+2+//fYrBaT7779/ubPs7rzzzlJ4Xtev4SpVqmSzzTYrfb9o0aJ1OBoAvi7BB+B7oKysLM8++2yOPvro0r+u16lTJzfeeOPXWqy1fKHP5a1Hc88995S+XtrVXpo2bZok+fe//71Sa+isbddcc81SZ9+89957ueyyy0rfn3rqqUts06RJk9I6SC+99NJSZxdMnz4955133gpP+apfv35pxtD48eNXe5bB3nvvnW233TbJlzN4rr322qX+3vr27VuKUZtuumn+8z//c7Wed0048sgjS1d6e/TRRzNgwIAltlm0aFGuuOKKvP7660m+nFGxxx57fKPj/CZVnDFy/fXXL/X10a9fvwwcOPAbG9ODDz6YRx55ZLmz5F544YXSqZPNmjWrtOjwmjymc889t7Tvyy+/PEOHDl3mth988EHpdVNR+d+mDz74IDNnzlzhcy5P06ZNc8ghhyT58qpbv/rVr5Yakp588slScK1Zs2ZOPvnk1Xre5RkyZEjuvffe5S4K/eabb5b+v2LDDTdcInID8N1g0WaAgnjyySdLX5eVlWXOnDmZOXNm3njjjYwePTqTJk0q3d+4ceNcd911pRCwqkaOHJlevXqlUaNG2WuvvdK8efM0atQoixcvztSpU/P000/npZdeSvLlaRpLiyR77LFH3nzzzcydOzddu3ZN586d06BBg9KHtZ122mmVLxf/dXXq1CkvvvhiTjjhhHTu3Dm77757qlatmldffTV/+ctfSrOQDjjggGX+y/tpp52Wyy+/PMmXHzqPOuqotG7dOmVlZRk/fnwGDhyYWbNm5aCDDsrjjz++3PG0a9cuQ4YMyXvvvZfzzz8/BxxwQKXTxtq0abPSVyurWrVqrr322hx//PH54osvcvvtt2fkyJE5/PDDs8kmm+STTz7JY489VvpwV7169VxzzTUrdcWota1OnTrp3r17zjrrrCxatChXXnllhgwZkoMOOigNGjTIlClT8vDDD+fNN99M8uUit9dcc806HvXa9V//9V8ZMGBAFixYkEGDBuWdd97JIYccko033jhTp07NY489lnHjxqVJkybZdNNNS+/Dtemtt97KgAED8rvf/S7t27fPjjvumMaNG2e99dbLxx9/nJEjR1YKL2eeeeZaO6a99torXbt2zZ/+9KfMnTs3Z555Ztq1a5dOnTqlcePGWbhwYSZNmpTRo0dn5MiRufjii0vrjZVr165dRo8enYULF+ass87Ksccem4YNG5b+NrVo0aIUIlfG5Zdfnpdeeikffvhhhg4dmkMPPTRHHXVUfvCDH2TOnDl57rnn8sQTT5S2v+SSS7LFFlus9P5X1eTJk9O9e/d07949e+yxR1q2bJmmTZumZs2amT59esaNG5cnnniidDn3M844I9WqVVtr4wFg7RF8AAri7LPPXuE29erVyxFHHJFzzz039erV+9rPVf7BZ9q0aRk0aNAyt2vQoEF69Oix1LB02mmn5W9/+1umT5+e4cOHZ/jw4ZXuHzBgQNq2bfu1x7gqWrZsmUMPPTS//vWv8+CDD1Za46hcp06d0qNHj2Xu4+ijj85LL72UQYMGZcGCBbn//vtz//33l+6vXr16rrjiilSrVm2Fwefss8/O888/ny+++CKPP/74Ets/9dRTpVkIK2OHHXZI//7984tf/CLTpk3La6+9VlorpKL69eunR48e39jPfWV06tQpPXv2zKWXXprZs2dn1KhRGTVq1BLbbbbZZrnpppu+1hXnvku22GKLXHXVVbnsssuyYMGCvPLKK3nllVcqbbP55punT58+ueGGG76RMZXPEvzss89Kl2Ffmpo1a+aSSy5ZIpqu6WM6//zzU79+/fzxj3/MvHnzMmLEiGWearm0GY5dunTJX/7yl0yZMiVjxozJmDFjKt3fu3fv7LfffiscR7kNN9ww99xzT37+85/njTfeyOTJk9OrV68ltqtRo0YuvfTSdOnSZaX3/XWUH/O8efMydOjQZc6CqlatWs4444ylBnsAvhsEH4ACql69emrXrp06deqkSZMmadGiRXbaaafss88+Kz0zZHluvvnmvPTSSxk+fHjGjRuX9957r3TqQ/369bPNNtukY8eOOeaYY5YZljbZZJMMGjQo/fr1y4gRIzJp0qR8/vnn62wR5yOOOCLNmzfPXXfdlREjRmTq1KnZYIMN0rx58xxzzDE5/PDDl/v4KlWqpHv37unQoUMeeOCBjB8/Pp9//nkaNWqUdu3a5eSTT06zZs1W6rSU7bffPgMHDswdd9xRmhmwuuuJtGrVKk888UQeeOCBPPXUU3n77bcze/bs1K5dO1tvvXX23nvvnHDCCasVAteW/fbbL0OGDMk999yTZ599NhMnTsycOXNSr169bLvtttl3331z3HHHrZHX9nfBf/7nf2bbbbfNbbfdltGjR2f69OmpU6dONt988+y///454YQTKl2ifG276KKLcsABB2TEiBF5+eWX8+9//zuffPJJFi1alDp16mTrrbdOu3btcuyxx1Za22ttHtMpp5ySgw8+OPfdd19eeOGFvPfee5k9e3Zq1KiRJk2apGXLltl3332z9957L/HYBg0a5KGHHkq/fv3ywgsv5P3338/cuXNX629TkyZNMnDgwDzyyCN5/PHH89prr2XGjBlZf/3106RJk7Rv3z5dunRZ5s9nTfrJT36SXXfdNSNGjMjo0aMzYcKETJs2LQsWLEjt2rWz+eabp3Xr1jn22GMrrU8GwHdPlbJvw+VRAAAAAFhjLNoMAAAAUDCCDwAAAEDBCD4AAAAABSP4AAAAABSM4AMAAABQMIIPAAAAQMEIPgAAAAAFI/gAAAAAFIzgAwAAAFAwgg8AAABAwQg+AAAAAAUj+AAAAAAUjOADAAAAUDDrresBAN8tt912W4YMGZJ33nknZWVl2XbbbXPWWWelY8eOlba7++67c/fdd2fy5MmpW7du2rdvn4suuigbbbTREvv8+OOPc8QRR+Tjjz/Os88+m8aNGydJpkyZkssvvzxvvfVWZs6cmfr162fPPffML3/5y9I2SXLllVdm3Lhxeeutt7Lv/EKlAAAa+0lEQVRgwYK8/vrrSzzHj370o0yePLnSbbvuumvuvffeNfFjAQAA+FYxwwdYJSNGjMjRRx+dAQMG5IEHHsguu+ySrl275uWXXy5t89hjj+Wqq67Kqaeemr///e+54YYb8tprr+WSSy5ZYn+LFy/OhRdemJYtWy5xX7Vq1XLAAQekb9++eeKJJ3LDDTfk3XffTdeuXZfYx2GHHZYTTjhhuWM/44wzMmzYsNL//vSnP33NnwIAAMC3mxk+wCq57bbbKn1/ySWXZNiwYRkyZEh22223JMmYMWPSrFmzHHvssUmSpk2b5sc//nF69uy5xP769OmT9dZbL6ecckqeeeaZSvdtvPHG+a//+q/S95tttll+9rOf5eyzz87s2bNTt27dJMl///d/J0kGDhy43LHXqlUrjRo1WsUjBgAA+O4xwwdYLYsXL86cOXPSoEGD0m277bZbJkyYkJEjR6asrCzTpk3L4MGD06lTp0qPHTFiRB544IFcc801qVKlygqfa/r06fnb3/6WFi1alGLPqrj77rvTtm3bHHroofnDH/6QGTNmrPI+AAAAvgvM8AFWS9++fTNr1qwcccQRpdsOOuigzJ49O2eccUYWLVqUhQsXZp999slVV11V2ubjjz/ORRddlKuvvjoNGzbM22+/vczn+OUvf5mnnnoqX3zxRVq1arXELKOVceKJJ2b77bfPhhtumLfffjs33nhjnn/++Tz88MNZf/31V3l/AAAA32Zm+ABf2913352bb745PXv2rLSI8ksvvZTrr78+l1xySR566KHccsstmTRpUrp161ba5sILL0znzp3Tvn37FT5Pt27dMmjQoNx6661JkgsuuCCLFi1apbGeeuqpadeuXbbbbrsccsghufXWWzNx4sQMGTJklfYDAADwXWCGD/C19OvXL7169cqf/vSn7LnnnpXuu/7667P//vunS5cuSZLmzZundu3a6dKlS84999xsueWWGT58eEaNGpV+/folScrKypJ8eTWtY445JldccUVpf40aNUqjRo3ygx/8IM2bN0+HDh3ywgsvLHFlsFWxxRZbpGHDhktcuQsAAKAIBB9gld14443p379/brnllrRp02aJ+z///PNUrVp5AmH59+Vh55FHHql0/6uvvprLLrss/fr1yw9+8INlPnf54+fPn79ax/DRRx9l+vTplWYmAQAAFIXgA6ySK6+8Mvfff3/++Mc/Zuutt860adOSJOuvv35pIeV99903t9xyS3baaafsvvvu+eijj3LVVVelWbNm2WKLLZIk2223XaX9li+gvPXWW2eTTTZJkgwePDiff/55dthhh9SuXTsTJ04snT7Wrl270mMnTpyYuXPn5oMPPkiSjB8/PsmXs3hq166dsWPHZsyYMWnXrl3q16+fd955J9ddd1023XTT7L///mvxpwUAALBuCD7AKhkwYECS5Oyzz650+5FHHpmrr746SdK1a9dUq1Ytffv2zZQpU1KvXr20bds2v/rVr5aY+bM8NWrUSL9+/fLOO+9k3rx52WSTTdK+fftcf/31qVOnTmm7X//61xk1alTp+86dO5fG2rZt29SoUSNDhgzJzTffnLlz56Zx48Zp3759zjnnnNSuXftr/ywAAAC+raqUlZ8fwVLNnvtF3n7/w3U9DABY67bZvHHq1nLVOgCAIjDDZwXefv/DnPvHu9b1MABgrev5yxPTqtlW63oYAACsAS7LDgAAAFAwgg8AAABAwQg+AAAAAAUj+AAAAAAUjOADAAAAUDCCDwAAAEDBCD4AAAAABSP4AAAAABSM4AMAAABQMIIPAAAAQMEIPgAAAAAFI/gAAAAAFIzgAwAAAFAwgg8AAABAwQg+AAAAAAUj+AAAAAAUjOADAAAAUDCCDwAAAEDBCD4AAAAABSP4AAAAABSM4AMAAABQMIIPAAAAQMEIPgAAAAAFI/gAAAAAFIzgAwAAAFAwgg8AAABAwQg+AAAAAAUj+AAAAAAUjOADAAAAUDCCDwAAAEDBCD4AAAAABSP4AAAAABSM4AMAAABQMIIPAAAAQMEIPgAAAAAFI/gAAAAAFIzgAwAAAFAwgg8AAABAwQg+AAAAAAUj+AAAAAAUjOADAAAAUDCCDwAAAEDBCD4AAAAABSP4AAAAABSM4AMAAABQMIIPAAAAQMEIPgAAAAAFI/gAAAAAFIzgAwAAAFAwgg8AAABAwQg+AAAAAAUj+AAAAAAUjOADAAAAUDCCDwAAAEDBCD4AAAAABSP4AAAAABSM4AMAAABQMIIPAAAAQMEIPgAAAAAFI/gAAAAAFIzgAwAAAFAwgg8AAABAwQg+AAAAAAUj+AAAAAAUjOADAAAAUDCCDwAAAEDBCD4AAAAABSP4AAAAABSM4AMAAABQMIIPAAAAQMEIPgAAAAAFI/gAAAAAFIzgAwAAAFAwgg8AAABAwQg+AAAAAAUj+AAAAAAUjOADAAAAUDCCDwAAAEDBCD4AAAAABSP4AAAAABSM4AMAAABQMIIPAAAAQMEIPgAAAAAFI/gAAAAAFIzgAwAAAFAwgg8AAABAwQg+AAAAAAUj+AAAAAAUjOADAAAAUDCCDwAAAEDBCD4AAAAABSP4AAAAABSM4AMAAABQMIIPAAAAQMEIPgAAAAAFI/gAAAAAFIzgAwAAAFAwgg8AAABAwQg+AAAAAAUj+AAAAAAUjOADAAAAUDCCDwAAAEDBCD4AAAAABSP4AAAAABSM4AMAAABQMIIPAAAAQMEIPgAAAAAFI/gAAAAAFIzgAwAAAFAwgg8AAABAwQg+AAAAAAUj+AAAAAAUjOADAAAAUDCCDwAAAEDBCD4AAAAABSP4AAAAABSM4AMAAABQMIIPAAAAQMEIPgAAAAAFI/gAAAAAFIzgAwAAAFAwgg8AAABAwQg+AAAAAAUj+AAAAAAUjOADAAAAUDCCDwAAAEDBCD4AAAAABSP4AAAAABSM4AMAAABQMIIPAAAAQMEIPgAAAAAFI/gAAAAAFIzgAwAAAFAwgg8AAABAwQg+AAAAAAUj+AAAAAAUjOADAAAAUDCCDwAAAEDBCD4AAAAABSP4AAAAABSM4AMAAABQMIIPAAAAQMEIPgAAAAAFI/gAAAAAFIzgAwAAAFAwgg8AAABAwQg+AAAAAAUj+AAAAAAUjOADAAAAUDCCDwAAAEDBCD4AAAAABSP4AAAAABSM4AMAAABQMIIPAAAAQMEIPgAAAAAFI/gAAAAAFIzgAwAAAFAwgg8AAABAwQg+AAAAAAUj+AAAAAAUjOADAAAAUDCCDwAAAEDBCD4AAAAABSP4AAAAABSM4AMAAABQMIIPAAAAQMEIPgAAAAAFI/gAAAAAFIzgAwAAAFAwgg8AAABAwQg+AAAAAAUj+AAAAAAUjOADAAAAUDCCDwAAAEDBCD4AAAAABSP4AAAAABSM4AMAAABQMIIPAAAAQMEIPgAAAAAFI/gAAAAAFIzgAwAAAFAwgg8AAABAwQg+AAAAAAUj+AAAAAAUjOADAAAAUDCCDwAAAEDBCD4AAAAABSP4AAAAABSM4AMAAABQMIIPAAAAQMEIPgAAAAAFI/gAAAAAFIzgAwAAAFAwgg8AAABAwQg+AAAAAAUj+AAAAAAUjOADAAAAUDCCDwAAAEDBCD4AAAAABSP4AAAAABSM4AMAAABQMIIPAAAAQMEIPgAAAAAFI/gAAAAAFIzgAwAAAFAwgg8AAABAwQg+AAAAAAUj+AAAAAAUjOADAAAAUDCCDwAAAEDBCD4AAAAABSP4AAAAABSM4AMAAABQMIIPAAAAQMEIPgAAAAAFI/gAAAAAFIzgAwAAAFAwgg8AAABAwQg+AAAAAAUj+AAAAAAUjOADAAAAUDCCDwAAAEDBCD4AAAAABSP4AAAAABSM4AMAAABQMIIPAAAAQMEIPgAAAAAFI/gAAAAAFIzgAwAAAFAwgg8AAABAwQg+AAAAAAUj+AAAAAAUjOADAAAAUDCCDwAAAEDBCD4AAAAABSP4AAAAABSM4AMAAABQMIIPAAAAQMEIPgAAAAAFI/gAAAAAFIzgAwAAAFAwgg8AAABAwQg+AAAAAAUj+AAAAAAUjOADAAAAUDCCDwAAAEDBCD4AAAAABSP4AAAAABSM4AMAAABQMIIPAAAAQMEIPgAAAAAFI/gAAAAAFIzgAwAAAFAwgg8AAABAwQg+AAAAAAUj+AAAAAAUjOADAAAAUDCCDwAAAEDBCD4AAAAABSP4AAAAABSM4AMAAABQMIIPAAAAQMEIPgAAAAAFI/gAAAAAFIzgAwAAAFAwgg8AAABAwQg+AAAAAAUj+AAAAAAUjOADAAAAUDCCDwAAAEDBCD4AAAAABSP4AAAAABSM4AMAAABQMIIPAAAAQMEIPgAAAAAFI/gAAAAAFIzgAwAAAFAwgg8AAABAwQg+AAAAAAUj+AAAAAAUjOADAAAAUDCCDwAAAEDBCD4AAAAABSP4AAAAABSM4AMAAABQMIIPAAAAQMEIPgAAAAAFI/gAAAAAFIzgAwAAAFAwgg8AAABAwQg+AAAAAAUj+AAAAAAUjOADAAAAUDCCDwAAAEDBCD4AAAAABSP4AAAAABSM4AMAAABQMIIPAAAAQMEIPgAAAAAFI/gAAAAAFIzgAwAAAFAwgg8AAABAwQg+AAAAAAUj+AAAAAAUjOADAAAAUDCCDwAAAEDBCD4AAAAABSP4AAAAABSM4AMAAABQMIIPAAAAQMEIPgAAAAAFI/gAAAAAFIzgAwAAAFAwgg8AAABAwQg+AAAAAAUj+AAAAAAUjOADAAAAUDCCDwAAAEDBCD4AAAAABSP4AAAAABSM4AMAAABQMIIPAAAAQMEIPgAAAAAFI/gAAAAAFIzgAwAAAFAwgg8AAABAwQg+AAAAAAUj+AAAAAAUjOADAAAAUDCCDwAAAEDBCD4AAAAABSP4AAAAABSM4AMAAABQMIIPAAAAQMEIPgAAAAAFI/gAAAAAFIzgAwAAAFAwgg8AAABAwQg+AAAAAAUj+AAAAAAUjOADAAAAUDCCDwAAAEDBCD4AAAAABSP4AAAAABSM4AMAAABQMIIPAAAAQMEIPgAAAAAFI/gAAAAAFIzgAwAAAFAwgg8AAABAwQg+AAAAyzB8+PBsv/322X///Uu3LVy4MLfeemsOPPDAtGzZMgcccEDuvvvuSo978sknc8YZZ6R9+/bZeeedc+ihh+bOO+9MWVnZN30IwPfUeut6AAAAAN9GH3/8cS699NLsueeeee+990q39+rVK/fff3/+z//5P2nevHnGjh2b3/zmN6levXqOO+64JMmoUaOyyy675KyzzkqjRo0ycuTIXHHFFZk3b15+9rOfratDAr5HBB8AAICvWLx4cS688MJ06dIl8+bNqxR8Bg0alNNOO60062fzzTfPP//5z/Tt27cUfC677LJK+9t8880zfvz4PP7444IP8I1wShcAAMBX9OnTJ1WqVMnpp5++xH3z5s1LjRo1Kt22/vrrZ/LkyZk8efIy9zl79uw0aNBgjY8VYGkEHwAAgApGjBiR++67L9dee22qVl3yI1PHjh1z11135c0330xZWVleeeWVPPTQQ0mSqVOnLnWfI0eOzN///vd06dJlrY4doJzgAwAA8L+mT5+eiy66KFdeeWUaNWq01G0uv/zy7LjjjuncuXNatGiR8847L8ccc0ySpFq1aktsP27cuJx99tk555xz8qMf/Witjh+gnDV8AAAA/te//vWvTJ06NWeddVbptsWLF6esrCw77LBDrrnmmhx++OG58cYbM3/+/EyfPj0bb7xx7r333iRJ06ZNK+1v5MiR6dq1a84888x07dr1Gz0W4PtN8AEAAPhfLVu2zCOPPFLptnvuuSdDhw7NLbfckk033bR0e40aNdK4ceMkyd///ve0bt06G264Yen+oUOH5rzzzst5552X00477Zs5AID/JfgAAAD8r1q1amW77bardFvDhg1TvXr10u3//Oc/M3ny5LRo0SKffPJJ7rjjjowfP740yydJHnvssVx00UU544wzcvjhh2fatGlJvjzlq2IUAlhbBJ8V2Gbzxun5yxPX9TAAYK3bZvPG63oIAN8J8+fPT+/evfPee++levXqad26de677740a9astM0999yTBQsWpE+fPunTp0/p9iZNmuTpp59eF8MGvmeqlJWVla3rQQAAAACw5rhKFwAAAEDBCD4AAAAABSP4AAAAABSM4AMAAABQMIIPAAAAQMEIPgAAAAAFI/gAAAAAFIzgAwAAAFAwgg8AAABAwQg+AAAAAAUj+AAAAAAUjOADAAAAUDCCD5Ak6dWrV5o1a5af/vSnS9x37rnn5sQTT1yl/X3yySfp1atXJk2atMJtBw4cmGbNmi3xv/3333+VnnNt6N+/f3bYYYd1PQwAYA0q/++er/7vlFNOWafjuvLKK78V//0DFMN663oAwLfLsGHD8s9//jM77bTTau3nk08+yU033ZQ2bdqkadOmK/WYO++8M+uvv37p+5o1a67WGAAAlqVu3bq57bbblrgNoCgEH6Ckfv362WSTTdK3b9/06dPnG3/+li1bpnbt2iu17RdffFEpDgEArIpq1apll112Walt/XcH8F3klC6gkq5du+bpp5/Om2++udztxo8fn5NPPjk777xzWrdunV/96lf5+OOPkySTJk3K4YcfniQ56aSTStOkV0fHjh3zP//zP7npppvSoUOHtGnTJkny8ssvp2vXrtlrr73SqlWrdO7cOX//+98rPfb6669P+/btK922cOHCNGvWLPfee2/ptnnz5uW3v/1tdtttt7Rt2zZXX311Fi5cuFrjBgC+W8r/G+HOO+/MH/7wh7Rr1y6dO3dOkjz99NM55ZRT0q5du+y666758Y9/nBdffLHS4y+88MIcd9xxlW6bOHFimjVrlueee65028yZM3PBBRdkl112yV577ZVbbrll7R8c8L1ihg9QyUEHHZSePXumb9++uf7665e6zfTp03PiiSfmhz/8Ya677rrMmTMn1113XU499dQ89NBD2XjjjdOjR49ceOGF+c1vfpMWLVqs1HMvXry4UmCpVq1aqlSpUvr+4YcfznbbbZff//73WbRoUZJk8uTJ2W233XL88cenRo0aefnll3PxxRenatWqOfjgg1fp2K+99toMGjQov/zlL7P11lvn/vvvzz/+8Y9V2gcA8N3x1X/YqVatWunrW2+9NW3bts21116bsrKyJF/+o9a+++6b008/PVWqVMnQoUPz05/+NPfdd1923nnnVXruSy+9NGPHjs3ll1+ehg0b5rbbbsukSZOc0g6sMYIPUEnVqlXzs5/9LJdffnnOPffcbL311ktsc/vttydJ+vXrlzp16iT/r717CYmyi+M4/jN1JiuFJk1raNHFmAILwi5MFwvpAom4cGhVFA1EQ1FRWItqkWMSpNU4kNViQJDu4KpFUBNdoE1GdmFqE1QYNSXZiOSEM+8ifPDxsV6nDF7n/X5WM/9z5pxndof/+T/nSJo5c6Y8Ho9u3rypiooKo6Jnzpw5Iy6XLi0tNX33+/3yeDymZ2tubpbNZjNilZWVxudkMqnFixers7NTV69eTSnh09XVpStXrmjfvn3GgY0rVqzQhg0bRjwGAAAYO758+WLZlAqFQkYVcWFhoRoaGkztW7ZsMT4nEgktXbpUr1690rVr11JK+EQiEYXDYQUCAa1fv16StGTJEpWVlZHwATBqSPgAsKisrFQwGNT58+dVX19vae/o6NDy5cuNZI8kLViwQE6nU48ePVJFRcVvzdva2mpa5DidTlO72+02JXukH4u1QCCgcDisDx8+GJU/Q3/7byKRiOLxuMrLy41YZmamysvL1dLSkupfAQAA/3G5ubkKhUKm2OCNrtWrV1t+8/79ezU2Nurhw4eKRqNG5U8ikUhp7qdPnyojI0Nr1qwxYpMmTZLb7VYkEklpLAD4GRI+ACyysrLk9XpVV1enXbt2Wdqj0aiKi4st8fz8fHV3d//2vPPmzfvloc1TpkyxxGpqavTixQvt3LlTs2fP1sSJE9Xa2qp79+6lNHc0Gh12DofDkdI4AABgbMjMzFRJSYklPvCa19A1QX9/v3bs2KG+vj7t3btXM2bMUE5Ojk6dOqWenp6U5o5Go8rLy7NsZLHuADCaSPgAGFZ1dbXOnj2rCxcuWNoKCgr0+fNnS/zTp08jPq/ndww+z0eSent7dffuXdXW1ppe/Rqo8hlgt9v1/ft3U2xoYqqgoEDSj+vkB1cudXV1jcqzAwCAsWXouuP169d6+fKlQqGQ3G63Ef/27Zup30jXHbFYTPF43JT0Yd0BYDRxSxeAYdlsNm3fvl3Xr1/Xx48fTW0LFy7U/fv3TbtZHR0dxgHKkpSdnS3px81Xf0tfX5+SyaQxlyTFYjHduXPH1K+oqEjd3d3GLWKS9ODBA1Mfl8ul7Oxs3bp1y4j19/fr9u3bf+fhAQDAmDKQ2BmcoHn79q2ePHli6ldUVKR3794pHo8bsaHrjpKSEiUSCYXDYSPW09NjufELAP4EFT4AfmrTpk1qbm7W48ePjQMMJWnbtm26ePGivF6vvF6vent71dDQoLlz52rdunWSpOnTp2v8+PFqa2tTbm6usrKyhi2b/hOTJ0/W/PnzFQwGNWHCBEnSuXPnlJeXZ9ptW7Vqlex2uw4dOqStW7fqzZs3unz5smksh8Mhj8ej06dPa9y4cZo1a5YuXbpk2bUDAAD/T8XFxZo6daqOHz+uPXv2KBaLKRAIqLCw0NRv7dq1CgaDOnz4sKqqqvTs2TO1tbWZ+rhcLpWVleno0aP6+vWrcUvXr15tB4BUUeED4KdycnKMG6sGczgcamlpkc1m0/79+3Xs2DGVlpYqFAoZu152u121tbV6/vy5Nm/erOrq6r/yjI2NjZo2bZpqampUX1+vjRs3Wg6Nzs/P15kzZ9TZ2Smfz6cbN27o5MmTlrEOHjyoqqoqNTU16cCBA3I6nabbOAAAwP+X3W5XMBiUJO3evVtNTU3y+XxatGiRqZ/L5ZLf71d7e7t8Pp/a29tVV1dnGe/EiRNatmyZ/H6/jhw5opUrVxo3dgHAaMhIDhwtDwAAAAAAgLRAhQ8AAAAAAECaIeEDAAAAAACQZkj4AAAAAAAApBkSPgAAAAAAAGmGhA8AAAAAAECaIeEDAAAAAACQZkj4AAAAAAAApBkSPgAAAAAAAGmGhA8AAAAAAECa+QclcEHSH9gXkgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1440x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# using seaborns countplot to show distribution of questions in dataset\n",
    "fig, ax = plt.subplots()\n",
    "g = sns.countplot(df.Class, palette='viridis')\n",
    "g.set_xticklabels(['Not Fraud', 'Fraud'])\n",
    "g.set_yticklabels([])\n",
    "\n",
    "# function to show values on bars\n",
    "def show_values_on_bars(axs):\n",
    "    def _show_on_single_plot(ax):        \n",
    "        for p in ax.patches:\n",
    "            _x = p.get_x() + p.get_width() / 2\n",
    "            _y = p.get_y() + p.get_height()\n",
    "            value = '{:.0f}'.format(p.get_height())\n",
    "            ax.text(_x, _y, value, ha=\"center\") \n",
    "\n",
    "    if isinstance(axs, np.ndarray):\n",
    "        for idx, ax in np.ndenumerate(axs):\n",
    "            _show_on_single_plot(ax)\n",
    "    else:\n",
    "        _show_on_single_plot(axs)\n",
    "show_values_on_bars(ax)\n",
    "\n",
    "sns.despine(left=True, bottom=True)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "plt.title('Distribution of Transactions', fontsize=30)\n",
    "plt.tick_params(axis='x', which='major', labelsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17304750013189596"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "# print percentage of samples where target == 1\n",
    "(len(df.loc[df.Class==1])) / (len(df.loc[df.Class == 0])) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "# Separate input features and target\n",
    "y = df.Class\n",
    "X = df.drop('Class', axis=1)\n",
    "\n",
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique predicted labels:  [0]\n",
      "Test score:  0.9981461194910255\n"
     ]
    }
   ],
   "source": [
    "#TODO\n",
    "# Train a DummyClassifier to predict with 'most_frequent' strategy\n",
    "dummy = DummyClassifier(strategy='most_frequent').fit(X_train, y_train)\n",
    "dummy_pred = dummy.predict(X_test)\n",
    "\n",
    "# checking unique labels\n",
    "print('Unique predicted labels: ', (np.unique(dummy_pred)))\n",
    "\n",
    "# checking accuracy\n",
    "print('Test score: ', accuracy_score(y_test, dummy_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_imbalanced(y_test, lr_pred):\n",
    "    # Checking accuracy\n",
    "    print('Accuracy: ', accuracy_score(y_test, lr_pred))\n",
    "    # Checking unique values\n",
    "    predictions = pd.DataFrame(lr_pred)\n",
    "    print('Unique predicted labels:\\n',predictions[0].value_counts())\n",
    "    #recall score\n",
    "    print('Recall: ',recall_score(y_test, lr_pred))\n",
    "    #precision score\n",
    "    print('Precision: ', precision_score(y_test, lr_pred))\n",
    "    # f1 score\n",
    "    print('F1 score: ',f1_score(y_test, lr_pred))\n",
    "    # confusion matrix\n",
    "    print('ConfMat')\n",
    "    print(pd.DataFrame(confusion_matrix(y_test, lr_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "# Train a LogisticRegressio model on the training data\n",
    "lr = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n",
    " \n",
    "# Predict on test set\n",
    "lr_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9807589674447347\n",
      "Unique predicted labels:\n",
      " 0    69734\n",
      "1     1468\n",
      "Name: 0, dtype: int64\n",
      "Recall:  0.8712121212121212\n",
      "Precision:  0.07833787465940055\n",
      "F1 score:  0.14375000000000002\n",
      "ConfMat\n",
      "       0     1\n",
      "0  69717  1353\n",
      "1     17   115\n"
     ]
    }
   ],
   "source": [
    "evaluate_imbalanced(y_test, lr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.Class\n",
    "X = df.drop('Class', axis=1)\n",
    "\n",
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>264873</th>\n",
       "      <td>161634.0</td>\n",
       "      <td>-0.395578</td>\n",
       "      <td>1.489129</td>\n",
       "      <td>-0.833442</td>\n",
       "      <td>-0.224271</td>\n",
       "      <td>0.369444</td>\n",
       "      <td>-1.453886</td>\n",
       "      <td>0.796593</td>\n",
       "      <td>-0.060403</td>\n",
       "      <td>0.338270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.231624</td>\n",
       "      <td>0.955194</td>\n",
       "      <td>-0.172092</td>\n",
       "      <td>-0.041050</td>\n",
       "      <td>-0.313444</td>\n",
       "      <td>-0.174301</td>\n",
       "      <td>0.064657</td>\n",
       "      <td>-0.036960</td>\n",
       "      <td>2.74</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163821</th>\n",
       "      <td>116237.0</td>\n",
       "      <td>1.950487</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>-1.761814</td>\n",
       "      <td>1.232470</td>\n",
       "      <td>0.523175</td>\n",
       "      <td>-0.650657</td>\n",
       "      <td>0.504231</td>\n",
       "      <td>-0.200857</td>\n",
       "      <td>0.116805</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086306</td>\n",
       "      <td>0.326297</td>\n",
       "      <td>-0.068839</td>\n",
       "      <td>-0.416589</td>\n",
       "      <td>0.426044</td>\n",
       "      <td>-0.486299</td>\n",
       "      <td>-0.031266</td>\n",
       "      <td>-0.072543</td>\n",
       "      <td>38.44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72083</th>\n",
       "      <td>54557.0</td>\n",
       "      <td>1.105167</td>\n",
       "      <td>-0.166253</td>\n",
       "      <td>0.569520</td>\n",
       "      <td>0.681043</td>\n",
       "      <td>-0.259189</td>\n",
       "      <td>0.642792</td>\n",
       "      <td>-0.437034</td>\n",
       "      <td>0.356746</td>\n",
       "      <td>0.441417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009073</td>\n",
       "      <td>0.293023</td>\n",
       "      <td>-0.028688</td>\n",
       "      <td>-0.242206</td>\n",
       "      <td>0.389813</td>\n",
       "      <td>0.482852</td>\n",
       "      <td>0.010705</td>\n",
       "      <td>-0.008399</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196949</th>\n",
       "      <td>131771.0</td>\n",
       "      <td>1.805238</td>\n",
       "      <td>0.961264</td>\n",
       "      <td>-1.717212</td>\n",
       "      <td>4.094625</td>\n",
       "      <td>0.938666</td>\n",
       "      <td>-0.227785</td>\n",
       "      <td>0.152911</td>\n",
       "      <td>0.066753</td>\n",
       "      <td>-1.073784</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.137875</td>\n",
       "      <td>-0.450959</td>\n",
       "      <td>0.098530</td>\n",
       "      <td>-0.662272</td>\n",
       "      <td>-0.150154</td>\n",
       "      <td>-0.098852</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.017622</td>\n",
       "      <td>37.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126213</th>\n",
       "      <td>77959.0</td>\n",
       "      <td>0.835421</td>\n",
       "      <td>-1.191847</td>\n",
       "      <td>0.578455</td>\n",
       "      <td>0.586101</td>\n",
       "      <td>-1.236663</td>\n",
       "      <td>0.194617</td>\n",
       "      <td>-0.532404</td>\n",
       "      <td>0.061561</td>\n",
       "      <td>-0.734344</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.072349</td>\n",
       "      <td>-0.109154</td>\n",
       "      <td>-0.308356</td>\n",
       "      <td>0.011968</td>\n",
       "      <td>0.461350</td>\n",
       "      <td>-0.244810</td>\n",
       "      <td>0.031845</td>\n",
       "      <td>0.060910</td>\n",
       "      <td>237.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time        V1        V2        V3        V4        V5        V6  \\\n",
       "264873  161634.0 -0.395578  1.489129 -0.833442 -0.224271  0.369444 -1.453886   \n",
       "163821  116237.0  1.950487  0.002312 -1.761814  1.232470  0.523175 -0.650657   \n",
       "72083    54557.0  1.105167 -0.166253  0.569520  0.681043 -0.259189  0.642792   \n",
       "196949  131771.0  1.805238  0.961264 -1.717212  4.094625  0.938666 -0.227785   \n",
       "126213   77959.0  0.835421 -1.191847  0.578455  0.586101 -1.236663  0.194617   \n",
       "\n",
       "              V7        V8        V9  ...       V21       V22       V23  \\\n",
       "264873  0.796593 -0.060403  0.338270  ...  0.231624  0.955194 -0.172092   \n",
       "163821  0.504231 -0.200857  0.116805  ...  0.086306  0.326297 -0.068839   \n",
       "72083  -0.437034  0.356746  0.441417  ...  0.009073  0.293023 -0.028688   \n",
       "196949  0.152911  0.066753 -1.073784  ... -0.137875 -0.450959  0.098530   \n",
       "126213 -0.532404  0.061561 -0.734344  ... -0.072349 -0.109154 -0.308356   \n",
       "\n",
       "             V24       V25       V26       V27       V28  Amount  Class  \n",
       "264873 -0.041050 -0.313444 -0.174301  0.064657 -0.036960    2.74      0  \n",
       "163821 -0.416589  0.426044 -0.486299 -0.031266 -0.072543   38.44      0  \n",
       "72083  -0.242206  0.389813  0.482852  0.010705 -0.008399    1.00      0  \n",
       "196949 -0.662272 -0.150154 -0.098852 -0.000030  0.017622   37.89      0  \n",
       "126213  0.011968  0.461350 -0.244810  0.031845  0.060910  237.00      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate our training data back together\n",
    "X = pd.concat([X_train, y_train], axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    213245\n",
       "0    213245\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate minority and majority classes\n",
    "not_fraud = X[X.Class==0]\n",
    "fraud = X[X.Class==1]\n",
    "\n",
    "# upsample minority using resample and n_samples equal to the size of majority\n",
    "fraud_upsampled = resample(fraud,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(not_fraud), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "\n",
    "# combine majority and upsampled minority\n",
    "upsampled = pd.concat([not_fraud, fraud_upsampled])\n",
    "\n",
    "# check new class counts\n",
    "upsampled.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying logistic regression again with the balanced dataset\n",
    "y_train = upsampled.Class\n",
    "X_train = upsampled.drop('Class', axis=1)\n",
    "#TODO\n",
    "# Train a logistic regression on the train data\n",
    "upsampled = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n",
    "# predict on the test data\n",
    "upsampled_pred = upsampled.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9807589674447347\n",
      "Unique predicted labels:\n",
      " 0    69734\n",
      "1     1468\n",
      "Name: 0, dtype: int64\n",
      "Recall:  0.8712121212121212\n",
      "Precision:  0.07833787465940055\n",
      "F1 score:  0.14375000000000002\n",
      "ConfMat\n",
      "       0     1\n",
      "0  69717  1353\n",
      "1     17   115\n"
     ]
    }
   ],
   "source": [
    "evaluate_imbalanced(y_test, upsampled_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    360\n",
       "0    360\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# still using our separated classes fraud and not_fraud from above\n",
    "# TODO\n",
    "# downsample majority\n",
    "not_fraud_downsampled = resample(not_fraud,\n",
    "                                replace = False, # sample without replacement\n",
    "                                n_samples = len(fraud), # match minority n\n",
    "                                random_state = 27) # reproducible results\n",
    "\n",
    "# combine minority and downsampled majority\n",
    "downsampled = pd.concat([not_fraud_downsampled, fraud])\n",
    "\n",
    "# checking counts\n",
    "downsampled.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying logistic regression again with the undersampled dataset\n",
    "\n",
    "y_train = downsampled.Class\n",
    "X_train = downsampled.drop('Class', axis=1)\n",
    "\n",
    "undersampled = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n",
    "\n",
    "undersampled_pred = undersampled.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9758574197354007\n",
      "Unique predicted labels:\n",
      " 0    69387\n",
      "1     1815\n",
      "Name: 0, dtype: int64\n",
      "Recall:  0.8636363636363636\n",
      "Precision:  0.0628099173553719\n",
      "F1 score:  0.11710323574730355\n",
      "ConfMat\n",
      "       0     1\n",
      "0  69369  1701\n",
      "1     18   114\n"
     ]
    }
   ],
   "source": [
    "evaluate_imbalanced(y_test, undersampled_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('creditcard.csv')\n",
    "y = df.Class\n",
    "X = df.drop('Class', axis=1)\n",
    "\n",
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a random forest model\n",
    "rfc = RandomForestClassifier(n_estimators=10).fit(X_train, y_train)\n",
    "\n",
    "# predict on test set\n",
    "rfc_pred = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9995786635206876\n",
      "Unique predicted labels:\n",
      " 0    71092\n",
      "1      110\n",
      "Name: 0, dtype: int64\n",
      "Recall:  0.803030303030303\n",
      "Precision:  0.9636363636363636\n",
      "F1 score:  0.8760330578512396\n",
      "ConfMat\n",
      "       0    1\n",
      "0  71066    4\n",
      "1     26  106\n"
     ]
    }
   ],
   "source": [
    "evaluate_imbalanced(y_test,rfc_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
